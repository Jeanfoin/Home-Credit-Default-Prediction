{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29078b1-0ef2-4099-a01e-d99e9ec9994e",
   "metadata": {},
   "source": [
    "# **Model and features selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2125f-b25d-4053-95ec-fb28921a81fa",
   "metadata": {},
   "source": [
    "## **Table of contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c14aa-be35-4897-b962-6dabde683b47",
   "metadata": {},
   "source": [
    "* [Our goals](#Our-goals)\n",
    "* [Loading the data](#Loading-the-data)\n",
    "* [Setting up the pipeline](#Setting-up-the-pipeline)\n",
    "    * [Loading, joining the dataframes](#Loading,-joining-the-dataframes)\n",
    "    * [Pipeline preprocessing blocks](#Pipeline-preprocessing-blocks)\n",
    "* [Testing different pipeline setups](#Testing-different-pipeline-setups)\n",
    "    * [First model: manual imputation and encoding](#First-model:-manual-imputation-and-encoding)\n",
    "        * [Ordinal encoding](#Ordinal-encoding)\n",
    "        * [Target encoding](#Target-encoding)\n",
    "    * [Second model: no imputation for categorical variable](#Second-model:-no-imputation-for-categorical-variable)\n",
    "* [Recursive feature selection](#Recursive-feature-selection)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc00d9a4-4561-41b7-8043-2e9af4a45dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path(os.getcwd()).parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utilities import (\n",
    "    features_creation,\n",
    "    plot_utilities\n",
    ")\n",
    "\n",
    "from utilities.plot_utilities import Rstyle_spines\n",
    "from utilities.features_creation import (\n",
    "    compute_features_credit_card,\n",
    "    compute_features_previous,\n",
    "    compute_features_bureau,\n",
    "    compute_features_instal,\n",
    "    compute_features_pos_cash,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import gc\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    TargetEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7092c3d-ebde-497b-ad45-6b979a6d9772",
   "metadata": {},
   "source": [
    "## **Our goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46caa345-947f-4701-8505-ed14b833c4fa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f8d7dA; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "In this notebook, we'll aim to:\n",
    "\n",
    "* **Properly Set Up the Main Pipeline:** This will involve merging all our additional datasets and newly created features with the main dataframe.\n",
    "* **Test Different Models:** We'll evaluate model performance using 3-fold cross-validation and select the model that performs best according to the ROC-AUC metric.\n",
    "* **Apply Recursive Feature Selection:** We'll use this technique to reduce our set of features, which will make our model easier to deploy later on.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63f63c-55e4-4c7f-be26-973b06ae2690",
   "metadata": {},
   "source": [
    "## **Loading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d463a38-8bb7-427f-9dea-6534e9e016f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "From now on, we'll use the full training set stored in the train_data directory, rather than limiting ourselves to a sample. The training dataset was created in a previous notebook and represents 80% of the full dataset. Additionally, we'll use a stratified method to determine our 3 training folds for cross-validation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1348c9-c11a-4302-8628-d44a6f09d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"../app_data/application_train.parquet\")\n",
    "X_train = train.drop(\"TARGET\", axis=1)\n",
    "y_train = train[\"TARGET\"]\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68517db0-4500-47eb-bc26-0867fd553362",
   "metadata": {},
   "source": [
    "## **Setting up the pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d7974-e2f8-4c8f-bc01-ad13f72ed78d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "In this section, we will:\n",
    "* Create functions to:\n",
    "    * Efficiently load data using pyarrow to optimize time and memory usage.\n",
    "    * Handle the joining of all datasets and simplify the imputation of data from additional dataframes.\n",
    "* Develop custom classes and functions to set up the preprocessing pipeline, including tasks such as imputation, encoding, feature engineering, and more.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1930d-e112-450b-a2ca-44001f957783",
   "metadata": {},
   "source": [
    "### **Loading, joining the dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ea12d2-af84-4138-9bbb-59e391f2df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_df(\n",
    "    sk_id_curr: pd.Series, filename: str, dir_loc: str = \"../add_data/\"\n",
    ") -> pd.DataFrame:\n",
    "    tmp_ds = ds.dataset(dir_loc + filename)\n",
    "    tmp_table = tmp_ds.to_table(filter=(ds.field(\"SK_ID_CURR\").isin(sk_id_curr)))\n",
    "    df = tmp_table.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10487cb-7105-44d9-ab2d-6a19b8978d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_app(app_df: pd.DataFrame, additional_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sk_id_curr = pd.DataFrame(index=app_df.index)\n",
    "    pre_joined = sk_id_curr.join(additional_df, how=\"left\").fillna(0)\n",
    "\n",
    "    joined = app_df.join(pre_joined, how=\"left\")\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eceb3bca-4bbf-4ae3-bbbb-1ab5f24144ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compute_and_join_with_app(\n",
    "    app_df: pd.DataFrame,\n",
    "    filename: str,\n",
    "    compute_func: Callable[[pd.DataFrame], pd.DataFrame],\n",
    "    dir_loc: str = \"../add_data/\",\n",
    ") -> pd.DataFrame:\n",
    "    sk_id_curr = app_df.index\n",
    "    # Load previous application data set\n",
    "    additional_df = load_additional_df(sk_id_curr, filename, dir_loc)\n",
    "    # Compute the features\n",
    "    additional_features = compute_func(additional_df)\n",
    "    # Delete and gets memory back\n",
    "    del additional_df\n",
    "    gc.collect()\n",
    "    # Join the dataset to the main one\n",
    "    augmented_df = join_with_app(app_df, additional_features)\n",
    "    # return additional_features\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75c35e0-d4a2-4fe1-95c6-b67381f5e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dataframe(app_df: pd.DataFrame, dir: str = \"../add_data/\") -> pd.DataFrame:\n",
    "    full_features = load_compute_and_join_with_app(\n",
    "        app_df, \"credit_card_balance.parquet\", compute_features_credit_card\n",
    "    )\n",
    "    full_features = load_compute_and_join_with_app(\n",
    "        full_features, \"previous_application.parquet\", compute_features_previous\n",
    "    )\n",
    "\n",
    "    full_features = load_compute_and_join_with_app(\n",
    "        full_features, \"bureau.parquet\", compute_features_bureau\n",
    "    )\n",
    "\n",
    "    full_features = load_compute_and_join_with_app(\n",
    "        full_features, \"installments_payments.parquet\", compute_features_instal\n",
    "    )\n",
    "    full_features = load_compute_and_join_with_app(\n",
    "        full_features, \"POS_CASH_balance.parquet\", compute_features_pos_cash\n",
    "    )\n",
    "    return full_features.set_index(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42207a08-4722-4802-8768-f71f66a4af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinDataFrame(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "\n",
    "        X_ = get_full_dataframe(X_)\n",
    "\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca127336-6c98-4395-a2cd-aeb4a71d784b",
   "metadata": {},
   "source": [
    "### **Pipeline preprocessing blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bca28df-aac4-4b97-9b89-1da838c2c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_block_cols = [\n",
    "    \"APARTMENTS_AVG\",\n",
    "    \"BASEMENTAREA_AVG\",\n",
    "    \"YEARS_BEGINEXPLUATATION_AVG\",\n",
    "    \"YEARS_BUILD_AVG\",\n",
    "    \"COMMONAREA_AVG\",\n",
    "    \"ELEVATORS_AVG\",\n",
    "    \"ENTRANCES_AVG\",\n",
    "    \"FLOORSMAX_AVG\",\n",
    "    \"FLOORSMIN_AVG\",\n",
    "    \"LANDAREA_AVG\",\n",
    "    \"LIVINGAPARTMENTS_AVG\",\n",
    "    \"LIVINGAREA_AVG\",\n",
    "    \"NONLIVINGAPARTMENTS_AVG\",\n",
    "    \"NONLIVINGAREA_AVG\",\n",
    "    \"APARTMENTS_MODE\",\n",
    "    \"BASEMENTAREA_MODE\",\n",
    "    \"YEARS_BEGINEXPLUATATION_MODE\",\n",
    "    \"YEARS_BUILD_MODE\",\n",
    "    \"COMMONAREA_MODE\",\n",
    "    \"ELEVATORS_MODE\",\n",
    "    \"ENTRANCES_MODE\",\n",
    "    \"FLOORSMAX_MODE\",\n",
    "    \"FLOORSMIN_MODE\",\n",
    "    \"LANDAREA_MODE\",\n",
    "    \"LIVINGAPARTMENTS_MODE\",\n",
    "    \"LIVINGAREA_MODE\",\n",
    "    \"NONLIVINGAPARTMENTS_MODE\",\n",
    "    \"NONLIVINGAREA_MODE\",\n",
    "    \"APARTMENTS_MEDI\",\n",
    "    \"BASEMENTAREA_MEDI\",\n",
    "    \"YEARS_BEGINEXPLUATATION_MEDI\",\n",
    "    \"YEARS_BUILD_MEDI\",\n",
    "    \"COMMONAREA_MEDI\",\n",
    "    \"ELEVATORS_MEDI\",\n",
    "    \"ENTRANCES_MEDI\",\n",
    "    \"FLOORSMAX_MEDI\",\n",
    "    \"FLOORSMIN_MEDI\",\n",
    "    \"LANDAREA_MEDI\",\n",
    "    \"LIVINGAPARTMENTS_MEDI\",\n",
    "    \"LIVINGAREA_MEDI\",\n",
    "    \"NONLIVINGAPARTMENTS_MEDI\",\n",
    "    \"NONLIVINGAREA_MEDI\",\n",
    "    \"TOTALAREA_MODE\",\n",
    "]\n",
    "\n",
    "low_entropy_features = [\n",
    "    \"FLAG_DOCUMENT_11\",\n",
    "    \"FLAG_DOCUMENT_13\",\n",
    "    \"FLAG_DOCUMENT_9\",\n",
    "    \"FLAG_DOCUMENT_14\",\n",
    "    \"FLAG_CONT_MOBILE\",\n",
    "    \"FLAG_DOCUMENT_15\",\n",
    "    \"FLAG_DOCUMENT_19\",\n",
    "    \"FLAG_DOCUMENT_20\",\n",
    "    \"FLAG_DOCUMENT_21\",\n",
    "    \"FLAG_DOCUMENT_17\",\n",
    "    \"FLAG_DOCUMENT_7\",\n",
    "    \"FLAG_DOCUMENT_2\",\n",
    "    \"FLAG_DOCUMENT_4\",\n",
    "    \"FLAG_DOCUMENT_10\",\n",
    "    \"FLAG_DOCUMENT_12\",\n",
    "    \"FLAG_MOBIL\",\n",
    "]\n",
    "\n",
    "cat_features = [\n",
    "    \"NAME_CONTRACT_TYPE\",\n",
    "    \"CODE_GENDER\",\n",
    "    \"FLAG_OWN_CAR\",\n",
    "    \"FLAG_OWN_REALTY\",\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    \"NAME_INCOME_TYPE\",\n",
    "    \"NAME_EDUCATION_TYPE\",\n",
    "    \"NAME_FAMILY_STATUS\",\n",
    "    \"NAME_HOUSING_TYPE\",\n",
    "    \"OCCUPATION_TYPE\",\n",
    "    \"WEEKDAY_APPR_PROCESS_START\",\n",
    "    \"ORGANIZATION_TYPE\",\n",
    "    \"FONDKAPREMONT_MODE\",\n",
    "    \"HOUSETYPE_MODE\",\n",
    "    \"WALLSMATERIAL_MODE\",\n",
    "    \"EMERGENCYSTATE_MODE\",\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    \"CNT_CHILDREN\",\n",
    "    \"AMT_INCOME_TOTAL\",\n",
    "    \"AMT_CREDIT\",\n",
    "    \"AMT_ANNUITY\",\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    \"REGION_POPULATION_RELATIVE\",\n",
    "    \"DAYS_BIRTH\",\n",
    "    \"DAYS_EMPLOYED\",\n",
    "    \"DAYS_REGISTRATION\",\n",
    "    \"DAYS_ID_PUBLISH\",\n",
    "    \"OWN_CAR_AGE\",\n",
    "    \"HOUR_APPR_PROCESS_START\",\n",
    "    \"EXT_SOURCE_1\",\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"EXT_SOURCE_3\",\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\",\n",
    "    \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_60_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DAYS_LAST_PHONE_CHANGE\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_HOUR\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_DAY\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_WEEK\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_MON\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_QRT\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_YEAR\",\n",
    "]\n",
    "\n",
    "columns_to_drop = low_entropy_features + [\"CNT_FAM_MEMBERS\", \"OBS_30_CNT_SOCIAL_CIRCLE\"]\n",
    "\n",
    "ext_source = [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8353a1-dc24-448f-835c-cbf20c98cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveAnomalieRescaleAndDrop(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_.loc[X_[\"DAYS_EMPLOYED\"] == 365243, \"DAYS_EMPLOYED\"] = np.nan\n",
    "        X_[\"DAYS_EMPLOYED\"] = -X_[\"DAYS_EMPLOYED\"] / 365\n",
    "        X_[\"DAYS_BIRTH\"] = -X_[\"DAYS_BIRTH\"] / 365\n",
    "        X_[\"DAYS_REGISTRATION\"] = -X_[\"DAYS_REGISTRATION\"] / 365\n",
    "        X_[\"DAYS_ID_PUBLISH\"] = -X_[\"DAYS_ID_PUBLISH\"] / 365\n",
    "        X_.drop(self.columns_to_drop, axis=1, inplace=True)\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0566d98e-e566-4eb1-be86-492d57a85965",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"simple_imputer\", simple_imputer, numerical_features + housing_block_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9233a64-c052-448d-95ff-7648f2d2f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard_scaler\", StandardScaler(), housing_block_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa23ede-f9d6-4a25-bb16-1a441afbbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6ad1e9-6e00-4f34-afdd-0b0d4bb8513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCASelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_pca, n_components=0.9):\n",
    "        self.features_to_pca = features_to_pca\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.pca.fit(X[self.features_to_pca])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        pca_result = self.pca.transform(X_[self.features_to_pca])\n",
    "        X_ = X_.drop(columns=self.features_to_pca)\n",
    "        for i in range(pca_result.shape[1]):\n",
    "            X_[f\"PCA_HOUSING_{i+1}\"] = pca_result[f\"pca{i}\"]\n",
    "\n",
    "        return X_\n",
    "\n",
    "\n",
    "housing_reduction = ColumnTransformer(\n",
    "    transformers=[(\"pca\", PCASelection(housing_block_cols), housing_block_cols)],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1805c3bc-8616-4ae5-8ee5-1508b62596d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[\"INCOME_CHILDREN_RATIO\"] = X_[\"AMT_INCOME_TOTAL\"] / (\n",
    "            X_[\"CNT_CHILDREN\"] + 0.0001\n",
    "        )\n",
    "        X_[\"CREDIT_INCOME_RATIO\"] = X_[\"AMT_CREDIT\"] / (X_[\"AMT_INCOME_TOTAL\"] + 0.0001)\n",
    "        X_[\"CREDIT_ANNUITY_RATIO\"] = X_[\"AMT_CREDIT\"] / (X_[\"AMT_ANNUITY\"] + 0.0001)\n",
    "        X_[\"ANNUITY_INCOME_RATIO\"] = X_[\"AMT_ANNUITY\"] / (\n",
    "            X_[\"AMT_INCOME_TOTAL\"] + 0.0001\n",
    "        )\n",
    "        X_[\"INCOME_ANNUITY_DIFF\"] = X_[\"AMT_INCOME_TOTAL\"] - X_[\"AMT_ANNUITY\"]\n",
    "        X_[\"CREDIT_GOODS_RATIO\"] = X_[\"AMT_CREDIT\"] / (X_[\"AMT_GOODS_PRICE\"] + 0.0001)\n",
    "        X_[\"CREDIT_GOODS_DIFF\"] = X_[\"AMT_CREDIT\"] - X_[\"AMT_GOODS_PRICE\"] + 0.0001\n",
    "        X_[\"GOODS_INCOME_RATIO\"] = X_[\"AMT_GOODS_PRICE\"] / (\n",
    "            X_[\"AMT_INCOME_TOTAL\"] + 0.0001\n",
    "        )\n",
    "        X_[\"AVG_EXT_SOURCE\"] = (\n",
    "            X_[\"EXT_SOURCE_1\"] + X_[\"EXT_SOURCE_2\"] + X_[\"EXT_SOURCE_3\"]\n",
    "        ) / 3\n",
    "        X_[\"HARM_AVG_EXT_SOURCE\"] = (\n",
    "            X_[\"EXT_SOURCE_1\"] * X_[\"EXT_SOURCE_2\"] * X_[\"EXT_SOURCE_3\"]\n",
    "        ) / (X_[\"EXT_SOURCE_1\"] + X_[\"EXT_SOURCE_2\"] + X_[\"EXT_SOURCE_3\"] + 0.001)\n",
    "        X_[\"AVG_60_OBS_DEF\"] = (\n",
    "            X_[\"OBS_60_CNT_SOCIAL_CIRCLE\"] + X_[\"DEF_60_CNT_SOCIAL_CIRCLE\"]\n",
    "        ) / 2\n",
    "        X_[\"RATION_EMPLOYED_AGE\"] = X_[\"DAYS_BIRTH\"] / X_[\"DAYS_EMPLOYED\"]\n",
    "\n",
    "        # Combinining several datasets\n",
    "        X_[\"RATIO_TOTAL_CREDIT_INCOME\"] = (\n",
    "            X_[\"AMT_CREDIT\"]\n",
    "            + X_[\"PREVIOUS_DIFF_CREDIT_DOWN_PAYMENT_SUM\"]\n",
    "            + X_[\"BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM\"]\n",
    "        ) / (X_[\"AMT_INCOME_TOTAL\"] + 0.0001)\n",
    "\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2bde52c-51c9-46a6-b718-e8b8420a97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "poly_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"polynomial_transformer\",\n",
    "            polynomial_transformer,\n",
    "            [\n",
    "                \"EXT_SOURCE_1\",\n",
    "                \"EXT_SOURCE_2\",\n",
    "                \"EXT_SOURCE_3\",\n",
    "                \"CREDIT_INCOME_RATIO\",\n",
    "                \"ANNUITY_INCOME_RATIO\",\n",
    "                \"CREDIT_ANNUITY_RATIO\",\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137d94f-1ff6-4e4c-8811-9030b87ea68e",
   "metadata": {},
   "source": [
    "## **Testing different pipeline setups**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18634717-a8af-4cfa-828a-d8413b506fe3",
   "metadata": {},
   "source": [
    "### **First model: manual imputation and encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c8abf-f1bb-4ac4-83e5-5014f662b89d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "In this model, we will handle the imputation and encoding processes manually, rather than relying on algorithms to do so.\n",
    "\n",
    "* **Imputation:** We'll use simple imputation strategies for both numerical and categorical features. For numerical features, we'll use the median, and for categorical features, we'll impute with the most frequent category.\n",
    "* **Encoding:** Since most categories lack a natural order, one might assume that one-hot encoding would be the best approach. However, I've found that one-hot encoding often performs poorly in practice. Therefore, we'll experiment with two different encoding methods: Ordinal Encoding and Target Encoding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72da28c-1a8f-491e-b912-d0760c8f5deb",
   "metadata": {},
   "source": [
    "#### **Ordinal encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895cac94-e4a7-43a0-a2aa-5ca18687b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"simple_imputer\", simple_imputer, numerical_features + housing_block_cols),\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1962151-9c27-4f35-b306-5c637ff31fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"encoder\",\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "            cat_features,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f87050-f9ac-47ff-bec3-a252581cc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"joiner\", JoinDataFrame()),\n",
    "        (\"remover\", RemoveAnomalieRescaleAndDrop(columns_to_drop)),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"housing_reduction\", housing_reduction),\n",
    "        (\"feature_engineering\", FeatureEngineering()),\n",
    "        (\"poly_transformer\", poly_transformer),\n",
    "        (\"encoder\", encoder),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb90afbb-745a-4e08-aa2d-04ffaee4b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", verbose=0),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        class_weights=[1, len(y_train[y_train == 0]) / len(y_train[y_train == 1])],\n",
    "        verbose=0,\n",
    "    ),\n",
    "    \"Dummy\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ee30ea-3e1f-43dc-821a-1eae1f85ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "CPU times: user 2.77 s, sys: 1.2 s, total: 3.97 s\n",
      "Wall time: 12min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifiers_scores = list(dict())\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (key, classifiers[key])])\n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=skf,\n",
    "        scoring=\"roc_auc\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    classifiers_scores.append(\n",
    "        {\n",
    "            \"classifier\": key,\n",
    "            \"train_score\": np.mean(scores[\"train_score\"]),\n",
    "            \"test_score\": np.mean(scores[\"test_score\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_scores_first = pd.DataFrame.from_records(classifiers_scores, index=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9951731c-7889-404c-b109-d12caf3490a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.823859</td>\n",
       "      <td>0.759089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.916471</td>\n",
       "      <td>0.747255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_score  test_score\n",
       "classifier                         \n",
       "LightGBM       0.823859    0.759089\n",
       "CatBoost       0.916471    0.747255\n",
       "Dummy          0.500000    0.500000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe726dd-a168-40f5-a4a2-35ad4e240c2b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "This time, we decided to drop XGBoost, as it was already performing worse than LightGBM and CatBoost. Our first observation is that our models, when augmented with data from additional datasets, are performing better. LightGBM's performance improved from 0.739356 to around 0.758 on the validation folds, while CatBoost improved from 0.737160 to around 0.747. Although this improvement may seem marginal, it could represent millions of dollars for a company like Home Credit.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084b397-9db0-46cf-a3b0-a24538b9b9b8",
   "metadata": {},
   "source": [
    "#### **Target encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f68330e-3408-4a17-94fe-9e1b8bb680c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"simple_imputer\", simple_imputer, numerical_features + housing_block_cols),\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "096dd9ad-f2a5-46ca-9df6-751eaee87528",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f75687-1e2e-40c3-b7a9-851718937732",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"joiner\", JoinDataFrame()),\n",
    "        (\"remover\", RemoveAnomalieRescaleAndDrop(columns_to_drop)),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"housing_reduction\", housing_reduction),\n",
    "        (\"feature_engineering\", FeatureEngineering()),\n",
    "        (\"poly_transformer\", poly_transformer),\n",
    "        (\"target_encoder\", target_encoder),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4528b36d-08f7-49ae-b18b-d2187b4de3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", verbose=0),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        class_weights=[1, len(y_train[y_train == 0]) / len(y_train[y_train == 1])],\n",
    "        verbose=0,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d93c8ce-a8e4-4c05-8cba-1bfbe47b00ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n"
     ]
    }
   ],
   "source": [
    "classifiers_scores = list(dict())\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (key, classifiers[key])])\n",
    "    scores = cross_validate(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=skf,\n",
    "        scoring=\"roc_auc\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    classifiers_scores.append(\n",
    "        {\n",
    "            \"classifier\": key,\n",
    "            \"train_score\": np.mean(scores[\"train_score\"]),\n",
    "            \"test_score\": np.mean(scores[\"test_score\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_scores_second = pd.DataFrame.from_records(classifiers_scores, index=\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cabeb4b3-2b0e-4fd3-8a06-9fab45fceea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.821973</td>\n",
       "      <td>0.758076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.906729</td>\n",
       "      <td>0.747803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_score  test_score\n",
       "classifier                         \n",
       "LightGBM       0.821973    0.758076\n",
       "CatBoost       0.906729    0.747803"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970ab02-46b9-45de-b649-1a07123033aa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "Target encoding seems to have improved the performance of both models. Once again, LightGBM is outperforming CatBoost, so we'll keep LightGBM and set aside CatBoost. Before proceeding with recursive feature selection, we'll try one last preprocessing option: allowing LightGBM to handle categorical variable imputation directly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7714fa6-127a-4c3b-9749-4cf834215fab",
   "metadata": {},
   "source": [
    "### **Second model: no imputation for categorical variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8270f115-eccd-468c-8e31-7579b85bc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "second_imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"simple_imputer\", simple_imputer, numerical_features + housing_block_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48fccc00-61b3-4edb-865e-dab03085aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"encoder\", TargetEncoder(target_type=\"binary\"), cat_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54eafeb0-381a-4cb2-b54c-273242f1b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"joiner\", JoinDataFrame()),\n",
    "        (\"remover\", RemoveAnomalieRescaleAndDrop(columns_to_drop)),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"housing_reduction\", housing_reduction),\n",
    "        (\"feature_engineering\", FeatureEngineering()),\n",
    "        (\"poly_transformer\", poly_transformer),\n",
    "        (\"target_encoder\", target_encoder),\n",
    "        (\"lightgbm\", LGBMClassifier(class_weight=\"balanced\", verbose=0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1db824b7-e3e5-4d14-8fa6-a020ece1e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(\n",
    "    second_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=skf,\n",
    "    scoring=\"roc_auc\",\n",
    "    return_train_score=True,\n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eccef5fe-c827-4cf0-afbd-df0c39135a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score = 0.8218217017952489 \n",
      "test_score = 0.7580602892518558\n"
     ]
    }
   ],
   "source": [
    "train_score = scores[\"train_score\"].mean()\n",
    "test_score = scores[\"test_score\"].mean()\n",
    "print(f\"train_score = {train_score} \\ntest_score = {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73557f1-b903-4c8f-8662-488c0b319cf3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "The performance is very slightly below what we achieved using our own imputation method for categorical variables. However, I’ll keep it this way, as I believe that combining this approach with hyperparameter tuning could lead to better overall performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f699f-9576-401c-af15-f1ce9ec3d04f",
   "metadata": {},
   "source": [
    "## **Recursive feature selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f32452-df2e-4f8b-a6fc-3a51f3f176a4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "Now we will proceed with recursive feature selection. By removing irrelevant features, we aim to improve model interpretability, make the deployed model more manageable, and likely increase its speed and efficiency.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dcbeece-f835-4604-99d6-bcee2c267ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(class_weight=\"balanced\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3161ea6e-0006-452e-84fc-c9717dc0ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(estimator=lgbm, step=5, cv=skf, scoring=\"roc_auc\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4db5543-23a6-4033-a4f0-015bbddc1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"joiner\", JoinDataFrame()),\n",
    "        (\"remover\", RemoveAnomalieRescaleAndDrop(columns_to_drop)),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"housing_reduction\", housing_reduction),\n",
    "        (\"feature_engineering\", FeatureEngineering()),\n",
    "        (\"poly_transformer\", poly_transformer),\n",
    "        (\"target_encoder\", target_encoder),\n",
    "        (\"rfcev\", rfecv),\n",
    "        (\"lightgbm\", lgbm),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4df99e99-f886-4e2d-8c9f-eb9ae47ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "%%time\n",
    "fsp_fitted = feature_selection_pipeline.fit(X_train, y_train)\n",
    "joblib.dump(fsp_fitted, '../pkl/fsp_fitted.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a0727b1-f394-4c41-a8cb-429b5c165ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR',\n",
       "       'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',\n",
       "       'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE',\n",
       "       'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE',\n",
       "       'WALLSMATERIAL_MODE', 'EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
       "       'EXT_SOURCE_3', 'CREDIT_INCOME_RATIO', 'ANNUITY_INCOME_RATIO',\n",
       "       'CREDIT_ANNUITY_RATIO', 'EXT_SOURCE_1 EXT_SOURCE_2',\n",
       "       'EXT_SOURCE_1 EXT_SOURCE_3', 'EXT_SOURCE_1 CREDIT_INCOME_RATIO',\n",
       "       'EXT_SOURCE_1 ANNUITY_INCOME_RATIO',\n",
       "       'EXT_SOURCE_1 CREDIT_ANNUITY_RATIO', 'EXT_SOURCE_2 EXT_SOURCE_3',\n",
       "       'EXT_SOURCE_2 CREDIT_INCOME_RATIO',\n",
       "       'EXT_SOURCE_2 ANNUITY_INCOME_RATIO',\n",
       "       'EXT_SOURCE_2 CREDIT_ANNUITY_RATIO',\n",
       "       'EXT_SOURCE_3 CREDIT_INCOME_RATIO',\n",
       "       'EXT_SOURCE_3 ANNUITY_INCOME_RATIO',\n",
       "       'EXT_SOURCE_3 CREDIT_ANNUITY_RATIO',\n",
       "       'CREDIT_INCOME_RATIO ANNUITY_INCOME_RATIO',\n",
       "       'CREDIT_INCOME_RATIO CREDIT_ANNUITY_RATIO', 'PCA_HOUSING_1',\n",
       "       'PCA_HOUSING_2', 'PCA_HOUSING_3', 'PCA_HOUSING_4', 'PCA_HOUSING_5',\n",
       "       'PCA_HOUSING_6', 'PCA_HOUSING_7', 'PCA_HOUSING_8', 'PCA_HOUSING_9',\n",
       "       'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
       "       'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
       "       'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE',\n",
       "       'HOUR_APPR_PROCESS_START', 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
       "       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
       "       'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       "       'AMT_REQ_CREDIT_BUREAU_YEAR', 'FLAG_WORK_PHONE',\n",
       "       'REGION_RATING_CLIENT_W_CITY', 'REG_CITY_NOT_LIVE_CITY',\n",
       "       'FLAG_DOCUMENT_3',\n",
       "       'CRE_BAL_PAYMENT_RECEIVABLE_RATIO_LAST_YEAR_SUM_SUM',\n",
       "       'PREVIOUS_AMT_ANNUITY_SUM', 'PREVIOUS_AMT_MISSING_CREDIT_MEAN',\n",
       "       'PREVIOUS_AMT_CREDIT_GOODS_DIFF_SUM',\n",
       "       'PREVIOUS_AMT_CREDIT_GOODS_DIFF_MAX',\n",
       "       'PREVIOUS_DIFF_CREDIT_DOWN_PAYMENT_MEAN',\n",
       "       'PREVIOUS_AMT_INTEREST_LOAN_MEAN',\n",
       "       'PREVIOUS_DAYS_TERMINATION_MEAN', 'PREVIOUS_LOANS_DURATION_MAX',\n",
       "       'PREVIOUS_LOANS_DURATION_MIN', 'BUREAU_ACTIVE_DAYS_CREDIT_MIN',\n",
       "       'BUREAU_ACTIVE_DAYS_CREDIT_MAX',\n",
       "       'BUREAU_ACTIVE_AMT_CREDIT_SUM_MAX',\n",
       "       'BUREAU_ACTIVE_EFFECTIVE_CREDIT_DURATION_MEAN',\n",
       "       'BUREAU_ACTIVE_EFFECTIVE_CREDIT_DURATION_MIN',\n",
       "       'BUREAU_ACTIVE_EFFECTIVE_CREDIT_DURATION_MAX',\n",
       "       'BUREAU_ACTIVE_RATIO_CURRENT_DEBT_CREDIT_MEAN',\n",
       "       'BUREAU_ACTIVE_RATIO_CURRENT_DEBT_CREDIT_MAX',\n",
       "       'BUREAU_ACTIVE_RATIO_AMT_CREDIT_LIMIT_MEAN',\n",
       "       'BUREAU_ACTIVE_RATIO_AMT_CREDIT_LIMIT_MAX',\n",
       "       'INSTAL_DIFF_DAYS_INSTALMENT_PAYMENT_EVER_SUM_MEAN',\n",
       "       'INSTAL_DIFF_DAYS_INSTALMENT_PAYMENT_EVER_SUM_SUM',\n",
       "       'INSTAL_DIFF_DAYS_INSTALMENT_PAYMENT_EVER_SUM_MAX',\n",
       "       'INSTAL_DIFF_DAYS_INSTALMENT_PAYMENT_LAST_YEAR_SUM_MEAN',\n",
       "       'INSTAL_DIFF_DAYS_INSTALMENT_PAYMENT_LAST_3_MONTHS_SUM_MEAN',\n",
       "       'INSTAL_NUM_INSTALMENT_NUMBER_MAX_MEAN',\n",
       "       'INSTAL_DAYS_ENTRY_PAYMENT_MAX_MAX',\n",
       "       'INSTAL_DAYS_ENTRY_PAYMENT_MIN_STD',\n",
       "       'INSTAL_AMT_PAYMENT_MEAN_MEAN', 'INSTAL_AMT_PAYMENT_MEAN_MAX',\n",
       "       'POS_CASH_CNT_INSTALMENT_MAX_MEAN',\n",
       "       'POS_CASH_CNT_INSTALMENT_MAX_SUM', 'INCOME_CHILDREN_RATIO',\n",
       "       'INCOME_ANNUITY_DIFF', 'CREDIT_GOODS_RATIO', 'CREDIT_GOODS_DIFF',\n",
       "       'GOODS_INCOME_RATIO', 'AVG_EXT_SOURCE', 'HARM_AVG_EXT_SOURCE',\n",
       "       'AVG_60_OBS_DEF', 'RATION_EMPLOYED_AGE',\n",
       "       'RATIO_TOTAL_CREDIT_INCOME'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsp_fitted = joblib.load(\"../pkl/fsp_fitted.joblib\")\n",
    "rfecv_fitted = fsp_fitted.named_steps[\"rfcev\"]\n",
    "features_out = fsp_fitted.named_steps[\"target_encoder\"].get_feature_names_out()\n",
    "relevant_features = features_out[rfecv_fitted.support_]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81129fd7-603d-4ce2-8070-898fea20527a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px;\">\n",
    "We started with over 500 features, and after feature selection, we're down to 103—reducing the number of features by nearly fivefold! The next step will be to design new functions that compute only the necessary features. Following that, we'll train the model and tune its hyperparameters before deploying it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8a5e2-ad26-4bb4-8adf-b768753d7997",
   "metadata": {},
   "source": [
    "## **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097b22d-8f2b-4ef4-b12e-8e12fd9080ed",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f8d7dA; padding: 10px; border-radius: 5px;\">\n",
    "In this notebook, we have:\n",
    "\n",
    "* **Created Custom Functions and Classes:** Developed tools to handle data merging, feature engineering, and preprocessing for our classifier.\n",
    "* **Evaluated Different Boosted Models:** Assessed various models and chose LightGBM for our classification task. We found that incorporating additional datasets improved our model's performance.\n",
    "* **Used Recursive Feature Selection:** Reduced the number of features significantly without compromising model performance. We now have only a fifth of the total features initially computed. Our next step is to modify the model pipeline to account for this reduction.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone (env)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
